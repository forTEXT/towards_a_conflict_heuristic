{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4baddd14",
   "metadata": {},
   "source": [
    "# Towards a Conflict Heuristic (DH 2023)\n",
    "\n",
    "## 01. Preprocessing\n",
    "\n",
    "Last updated: 11.01.2023\n",
    "julian.haeussler[at]tu-darmstadt.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a4455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "!python -m spacy download de_core_news_lg\n",
    "nlp = spacy.load('de_core_news_lg',exclude=['ner'],disable=['tagger','parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c409dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = r'C:\\Users\\Public\\Data\\conflict_heuristics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3740b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = r'C:\\Users\\Public\\Data\\conflict_heuristics\\pickled\\all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0911d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f460700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "\n",
    "def extract_vp(title,corpus):\n",
    "    path = path_data+'\\\\Output_'+corpus+'\\\\'+corpus\n",
    "    \n",
    "    # read json file and extract text+spans\n",
    "    with open(path + \"\\\\\" + title + \".json\", 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        text = json_data[0]['text']\n",
    "        annotations = json_data[0]['annotations']\n",
    "\n",
    "    # create list of verb phrases\n",
    "    phrases = []\n",
    "    for i in range(0,len(annotations)):\n",
    "        start = annotations[i]['start']\n",
    "        end = annotations[i]['end']\n",
    "        phrases.append(text[start:end])\n",
    "            \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346804ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(phrases):\n",
    "    phrases_clean = []\n",
    "    # remove escape characters\n",
    "    for phrase in phrases:\n",
    "        phrases_clean.append(phrase.replace('\\n', ' '))\n",
    "    return phrases_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb915a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(phrases_clean):\n",
    "    phrases_nopunct = []\n",
    "    s = string.punctuation + '–' +'»' + '«'\n",
    "    # remove punctuation\n",
    "    for phrase in phrases_clean:\n",
    "        phrases_nopunct.append(phrase.translate(str.maketrans('', '', s)))\n",
    "    return phrases_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11fe64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spacy(phrases_nopunct):\n",
    "    phrases_lemmatized = [0]*len(phrases_nopunct)\n",
    "    phrases_tokenized = [0]*len(phrases_nopunct)\n",
    "    # run spacy and extract lemmas+tokens\n",
    "    for i in range(0, len(phrases_nopunct)):\n",
    "        words = nlp(phrases_nopunct[i])\n",
    "        interim_lem = [0]*len(words)\n",
    "        interim_tok = [0]*len(words)\n",
    "        for j in range(0, len(interim_lem)):\n",
    "            interim_lem[j] = words[j].lemma_\n",
    "            interim_tok[j] = words[j].text\n",
    "        phrases_lemmatized[i] = interim_lem\n",
    "        phrases_tokenized[i] = interim_tok\n",
    "    \n",
    "    return phrases_lemmatized, phrases_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a436013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_len(phrases):\n",
    "    phrases_filtered = []\n",
    "    for i in range(0,len(phrases)):\n",
    "        if len(phrases[i]) > 2:\n",
    "            phrases_filtered.append(phrases[i])\n",
    "    return phrases_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2dee786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(phrases):\n",
    "    phrases_lower = [[word.lower() for word in phrase] for phrase in phrases]\n",
    "    return phrases_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d3f3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_words(phrases):\n",
    "    words = [word for phrase in phrases for word in phrase]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28aae7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file,name):\n",
    "    with open(path_results + '\\\\' + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a65a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b507a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(title,corpus):\n",
    "    phrases = extract_vp(title,corpus)\n",
    "    phrases_cleaned = cleaning(phrases)\n",
    "    \n",
    "    phrases_nopunct = remove_punctuation(phrases_cleaned)\n",
    "    phrases_lemmatized, phrases_tokenized = run_spacy(phrases_nopunct)\n",
    "    \n",
    "    phrases_lemmatized_filtered = filter_len(phrases_lemmatized)\n",
    "    phrases_tokenized_filtered = filter_len(phrases_tokenized)\n",
    "    \n",
    "    phrases_lemmatized_final = lowercasing(phrases_lemmatized_filtered)\n",
    "    phrases_tokenized_final = lowercasing(phrases_tokenized_filtered)\n",
    "    \n",
    "    words_lemmatized_final = list_of_words(phrases_lemmatized_final)\n",
    "    words_tokenized_final = list_of_words(phrases_tokenized_final)\n",
    "    \n",
    "    save(phrases_lemmatized_final,title+\"_phrases_lemmatized_final\")\n",
    "    save(phrases_tokenized_final,title+\"_phrases_tokenized_final\")\n",
    "    save(words_lemmatized_final,title+\"_words_lemmatized_final\")\n",
    "    save(words_tokenized_final,title+\"_words_tokenized_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61956a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea3daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Romantik \n",
    "\n",
    "lst_files = glob.glob(os.path.join(os.getcwd(), path_data+'\\\\Output_Romantik\\\\Romantik', \"*.json\"))\n",
    "\n",
    "titles = []\n",
    "\n",
    "for entry in lst_files:\n",
    "    titles.append(re.search(r\"(?<=Output_Romantik\\\\Romantik\\\\)(.*)(?=.json)\",entry).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015da65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec06c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(titles)):\n",
    "    preprocessing(titles[i],\"Romantik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5075e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "455907c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Realismus\n",
    "\n",
    "lst_files = glob.glob(os.path.join(os.getcwd(), path_data+'\\\\Output_Realismus\\\\Realismus', \"*.json\"))\n",
    "\n",
    "titles = []\n",
    "\n",
    "for entry in lst_files:\n",
    "    titles.append(re.search(r\"(?<=Output_Realismus\\\\Realismus\\\\)(.*)(?=.json)\",entry).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97735b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a45447",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(titles)):\n",
    "    preprocessing(titles[i],\"Realismus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9469ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42f57b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Naturalismus\n",
    "\n",
    "lst_files = glob.glob(os.path.join(os.getcwd(), path_data+'\\\\Output_Naturalismus\\\\Naturalismus', \"*.json\"))\n",
    "\n",
    "titles = []\n",
    "\n",
    "for entry in lst_files:\n",
    "    titles.append(re.search(r\"(?<=Output_Naturalismus\\\\Naturalismus\\\\)(.*)(?=.json)\",entry).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e41edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19019275",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(titles)):\n",
    "    preprocessing(titles[i],\"Naturalismus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a60e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e2d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
